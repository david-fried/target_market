{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This ML script would not have been possible without inspiration from, github user ***'wiamsuri'*** and their project ***'cnn-image-classifier-keras'***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_files\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "# from PIL import ImageFile\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from keras.preprocessing import image\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.transform import resize\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Images From File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 image categories.\n",
      "Three categories:\n",
      "['brick_10/', 'siding_20/', 'unknown_00/']\n"
     ]
    }
   ],
   "source": [
    "image_names = [item.replace('resources/data_for_training_06/', '') for item in sorted(glob(\"resources/data_for_training_06/*/\"))]\n",
    "number_of_image_categories = len(image_names)\n",
    "print('%d image categories.' % number_of_image_categories)\n",
    "print('Three categories:')\n",
    "print(image_names[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4105 images.\n",
      "\n",
      "2463 training images.\n",
      "821 validation images.\n",
      "821 test images.\n"
     ]
    }
   ],
   "source": [
    "def load_dataset(path):\n",
    "    data = load_files(path)\n",
    "    image_files = np.array(data['filenames'])\n",
    "    image_targets = np_utils.to_categorical(np.array(data['target']), number_of_image_categories)\n",
    "    return image_files, image_targets\n",
    "\n",
    "\n",
    "image_files, image_targets = load_dataset('resources/data_for_training_06/')\n",
    "\n",
    "trains_validate_files, test_files, trains_validate_targets, test_targets = \\\n",
    "    train_test_split(image_files, image_targets, test_size=0.2, random_state=42)\n",
    "\n",
    "train_files, valid_files, train_targets, valid_targets = \\\n",
    "    train_test_split(trains_validate_files, trains_validate_targets, test_size=0.25, random_state=42)\n",
    "\n",
    "image_names = [item for item in sorted(glob(\"resources/data_for_training_06/*/\"))]\n",
    "\n",
    "print('%s images.\\n' % len(np.hstack([train_files, valid_files, test_files])))\n",
    "print('%d training images.' % len(train_files))\n",
    "print('%d validation images.' % len(valid_files))\n",
    "print('%d test images.'% len(test_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define function for preprocessing images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_to_tensor(img_path):\n",
    "\n",
    "    img = image.load_img(img_path, grayscale=False, color_mode=\"rgb\", target_size=(500, 500), interpolation=\"nearest\")\n",
    "    img_array = image.img_to_array(img)\n",
    "\n",
    "    return np.expand_dims(img_array, axis=0)\n",
    "\n",
    "def paths_to_tensor(img_paths):\n",
    "    \n",
    "    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n",
    "    return np.vstack(list_of_tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2463/2463 [00:36<00:00, 67.46it/s]\n",
      "100%|██████████| 821/821 [00:14<00:00, 54.97it/s]\n",
      "100%|██████████| 821/821 [00:10<00:00, 81.55it/s]\n"
     ]
    }
   ],
   "source": [
    "from PIL import ImageFile                            \n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True                 \n",
    "\n",
    "# pre-process the data for Keras\n",
    "train_tensors = paths_to_tensor(train_files).astype('float32')/255\n",
    "valid_tensors = paths_to_tensor(valid_files).astype('float32')/255\n",
    "test_tensors = paths_to_tensor(test_files).astype('float32')/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 500, 500, 4)       52        \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 250, 250, 4)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 250, 250, 8)       136       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 125, 125, 8)       0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 125, 125, 8)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 125, 125, 12)      396       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 62, 62, 12)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 62, 62, 12)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 62, 62, 16)        784       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 31, 31, 16)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 31, 31, 16)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 15376)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               3936512   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 771       \n",
      "=================================================================\n",
      "Total params: 3,938,651\n",
      "Trainable params: 3,938,651\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "# ADJUST FILTER?\n",
    "model.add(Conv2D(filters=4, kernel_size=2, padding='same',\n",
    "                 activation='relu', input_shape=(500, 500, 3)))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "\n",
    "model.add(Conv2D(filters=8, kernel_size=2, padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Conv2D(filters=12, kernel_size=2, padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(filters=16, kernel_size=2, padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model with training and validating images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "39/39 [==============================] - ETA: 0s - loss: 1.7239 - accuracy: 0.4710\n",
      "Epoch 00001: val_loss improved from inf to 0.95231, saving model to saved_models/weights.best.image_classifier.hdf5\n",
      "39/39 [==============================] - 196s 5s/step - loss: 1.7239 - accuracy: 0.4710 - val_loss: 0.9523 - val_accuracy: 0.4896\n",
      "Epoch 2/20\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.8981 - accuracy: 0.5644\n",
      "Epoch 00002: val_loss did not improve from 0.95231\n",
      "39/39 [==============================] - 221s 6s/step - loss: 0.8981 - accuracy: 0.5644 - val_loss: 0.9603 - val_accuracy: 0.5335\n",
      "Epoch 3/20\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.8457 - accuracy: 0.6248\n",
      "Epoch 00003: val_loss improved from 0.95231 to 0.89478, saving model to saved_models/weights.best.image_classifier.hdf5\n",
      "39/39 [==============================] - 228s 6s/step - loss: 0.8457 - accuracy: 0.6248 - val_loss: 0.8948 - val_accuracy: 0.6175\n",
      "Epoch 4/20\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.7757 - accuracy: 0.6630\n",
      "Epoch 00004: val_loss improved from 0.89478 to 0.83889, saving model to saved_models/weights.best.image_classifier.hdf5\n",
      "39/39 [==============================] - 230s 6s/step - loss: 0.7757 - accuracy: 0.6630 - val_loss: 0.8389 - val_accuracy: 0.6017\n",
      "Epoch 5/20\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.7203 - accuracy: 0.6918\n",
      "Epoch 00005: val_loss did not improve from 0.83889\n",
      "39/39 [==============================] - 237s 6s/step - loss: 0.7203 - accuracy: 0.6918 - val_loss: 0.8667 - val_accuracy: 0.6382\n",
      "Epoch 6/20\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.7015 - accuracy: 0.7073\n",
      "Epoch 00006: val_loss improved from 0.83889 to 0.77623, saving model to saved_models/weights.best.image_classifier.hdf5\n",
      "39/39 [==============================] - 248s 6s/step - loss: 0.7015 - accuracy: 0.7073 - val_loss: 0.7762 - val_accuracy: 0.7138\n",
      "Epoch 7/20\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.6344 - accuracy: 0.7316\n",
      "Epoch 00007: val_loss improved from 0.77623 to 0.76809, saving model to saved_models/weights.best.image_classifier.hdf5\n",
      "39/39 [==============================] - 234s 6s/step - loss: 0.6344 - accuracy: 0.7316 - val_loss: 0.7681 - val_accuracy: 0.7369\n",
      "Epoch 8/20\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.5888 - accuracy: 0.7609\n",
      "Epoch 00008: val_loss improved from 0.76809 to 0.63885, saving model to saved_models/weights.best.image_classifier.hdf5\n",
      "39/39 [==============================] - 222s 6s/step - loss: 0.5888 - accuracy: 0.7609 - val_loss: 0.6389 - val_accuracy: 0.7576\n",
      "Epoch 9/20\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.5490 - accuracy: 0.7698\n",
      "Epoch 00009: val_loss improved from 0.63885 to 0.61768, saving model to saved_models/weights.best.image_classifier.hdf5\n",
      "39/39 [==============================] - 232s 6s/step - loss: 0.5490 - accuracy: 0.7698 - val_loss: 0.6177 - val_accuracy: 0.7625\n",
      "Epoch 10/20\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.5044 - accuracy: 0.7986\n",
      "Epoch 00010: val_loss did not improve from 0.61768\n",
      "39/39 [==============================] - 223s 6s/step - loss: 0.5044 - accuracy: 0.7986 - val_loss: 0.6643 - val_accuracy: 0.7065\n",
      "Epoch 11/20\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.4615 - accuracy: 0.8140\n",
      "Epoch 00011: val_loss improved from 0.61768 to 0.57219, saving model to saved_models/weights.best.image_classifier.hdf5\n",
      "39/39 [==============================] - 225s 6s/step - loss: 0.4615 - accuracy: 0.8140 - val_loss: 0.5722 - val_accuracy: 0.7625\n",
      "Epoch 12/20\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.4143 - accuracy: 0.8335\n",
      "Epoch 00012: val_loss improved from 0.57219 to 0.57065, saving model to saved_models/weights.best.image_classifier.hdf5\n",
      "39/39 [==============================] - 232s 6s/step - loss: 0.4143 - accuracy: 0.8335 - val_loss: 0.5707 - val_accuracy: 0.7722\n",
      "Epoch 13/20\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.3998 - accuracy: 0.8473\n",
      "Epoch 00013: val_loss improved from 0.57065 to 0.54384, saving model to saved_models/weights.best.image_classifier.hdf5\n",
      "39/39 [==============================] - 222s 6s/step - loss: 0.3998 - accuracy: 0.8473 - val_loss: 0.5438 - val_accuracy: 0.7808\n",
      "Epoch 14/20\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.3329 - accuracy: 0.8680\n",
      "Epoch 00014: val_loss did not improve from 0.54384\n",
      "39/39 [==============================] - 219s 6s/step - loss: 0.3329 - accuracy: 0.8680 - val_loss: 0.6384 - val_accuracy: 0.7308\n",
      "Epoch 15/20\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.3028 - accuracy: 0.8831\n",
      "Epoch 00015: val_loss improved from 0.54384 to 0.53900, saving model to saved_models/weights.best.image_classifier.hdf5\n",
      "39/39 [==============================] - 222s 6s/step - loss: 0.3028 - accuracy: 0.8831 - val_loss: 0.5390 - val_accuracy: 0.7734\n",
      "Epoch 16/20\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.2735 - accuracy: 0.8940\n",
      "Epoch 00016: val_loss did not improve from 0.53900\n",
      "39/39 [==============================] - 222s 6s/step - loss: 0.2735 - accuracy: 0.8940 - val_loss: 0.5885 - val_accuracy: 0.7564\n",
      "Epoch 17/20\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.2594 - accuracy: 0.8997\n",
      "Epoch 00017: val_loss did not improve from 0.53900\n",
      "39/39 [==============================] - 239s 6s/step - loss: 0.2594 - accuracy: 0.8997 - val_loss: 0.5773 - val_accuracy: 0.7759\n",
      "Epoch 18/20\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.2271 - accuracy: 0.9131\n",
      "Epoch 00018: val_loss improved from 0.53900 to 0.53167, saving model to saved_models/weights.best.image_classifier.hdf5\n",
      "39/39 [==============================] - 222s 6s/step - loss: 0.2271 - accuracy: 0.9131 - val_loss: 0.5317 - val_accuracy: 0.7917\n",
      "Epoch 19/20\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.2016 - accuracy: 0.9237\n",
      "Epoch 00019: val_loss did not improve from 0.53167\n",
      "39/39 [==============================] - 250s 6s/step - loss: 0.2016 - accuracy: 0.9237 - val_loss: 0.6405 - val_accuracy: 0.7637\n",
      "Epoch 20/20\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.1781 - accuracy: 0.9298\n",
      "Epoch 00020: val_loss did not improve from 0.53167\n",
      "39/39 [==============================] - 209s 5s/step - loss: 0.1781 - accuracy: 0.9298 - val_loss: 0.6411 - val_accuracy: 0.7881\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.image_classifier.hdf5',\n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "history = model.fit(train_tensors, train_targets, validation_data=(valid_tensors, valid_targets),\n",
    "          epochs=epochs, batch_size=64, callbacks=[checkpointer], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 78.9281%\n"
     ]
    }
   ],
   "source": [
    "model.load_weights('saved_models/weights.best.image_classifier.hdf5')\n",
    "\n",
    "predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\n",
    "\n",
    "test_accuracy = 100*np.sum(np.array(predictions)==np.argmax(test_targets, axis=1))/len(predictions)\n",
    "print('Test accuracy: %.4f%%' % test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image in os.listdir('test_imgs'):\n",
    "    img = plt.imread('test_imgs' + '/' + image)\n",
    "    resized_image = resize(img, (500,500,3))\n",
    "    predictions = model.predict(np.array([resized_image]))\n",
    "#     print(f'{image}:\\t {predictions}')\n",
    "    if image.split(\"_\")[0] == \"brick\":\n",
    "        print(image, predictions)\n",
    "#         print(f'{image}:\\t {round(100 * predictions[0][0])}%')\n",
    "\n",
    "# for image in os.listdir('test_imgs'):\n",
    "#     img = plt.imread('test_imgs' + '/' + image)\n",
    "#     resized_image = resize(img, (400,400,3))\n",
    "#     predictions = model.predict(np.array([resized_image]))\n",
    "# #     print(f'{image}:\\t {predictions}')\n",
    "#     if image.split(\"_\")[0] == \"brick\":\n",
    "# #         print(f'{image}:\\t {round(100 * predictions[0][0])}%')\n",
    "\n",
    "# for image in os.listdir('test_imgs'):\n",
    "#     img = plt.imread('test_imgs' + '/' + image)\n",
    "#     resized_image = resize(img, (400,400,3))\n",
    "#     predictions = model.predict(np.array([resized_image]))\n",
    "# #     print(f'{image}:\\t {predictions}')\n",
    "#     if image.split(\"_\")[0] == \"unknown\":\n",
    "# #         print(f'{image}:\\t {round(100 * predictions[0][0])}%')\n",
    "\n",
    "#['brick_10/', 'siding_20/', 'unknown_00/']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_image = plt.imread('test_imgs/brick_7266.jpg')\n",
    "img = plt.imshow(new_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sort the predictions from least to greatest\n",
    "list_index = [0, 1, 2]\n",
    "x = predictions\n",
    "\n",
    "for i in range(3):\n",
    "  for j in range(3):\n",
    "    if x[0][list_index[i]] > x[0][list_index[j]]:\n",
    "      temp = list_index[i]\n",
    "      list_index[i] = list_index[j]\n",
    "      list_index[j] = temp\n",
    "\n",
    "#Show the sorted labels in order\n",
    "print(list_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## OPTIONAL WRITE HISTORY TO JSON\n",
    "# with open('trainHistoryDict', 'wb') as file_pi:\n",
    "#         pickle.dump(history.history, file_pi)\n",
    "\n",
    "# history = pickle.load(open('/trainHistoryDict', \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot history: \n",
    "plt.plot(history.history['loss'], label='(training data)')\n",
    "plt.plot(history.history['val_loss'], label='(validation data)')\n",
    "plt.title('Brick Guess')\n",
    "plt.ylabel('value')\n",
    "plt.xlabel('No. epoch')\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()\n",
    "plt.savefig('65pct.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
