{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This ML script would not have been possible without inspiration from, github user ***'wiamsuri'*** and their project ***'cnn-image-classifier-keras'***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_files\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "# from PIL import ImageFile\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from keras.preprocessing import image\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.transform import resize\n",
    "import os\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Images From File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 image categories.\n",
      "Three categories:\n",
      "['brick_10', 'siding_20', 'unknown_00']\n"
     ]
    }
   ],
   "source": [
    "image_names = sorted(os.listdir('resources/data_for_training_06'))\n",
    "number_of_image_categories = len(image_names)\n",
    "print('%d image categories.' % number_of_image_categories)\n",
    "print('Three categories:')\n",
    "print(image_names[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['brick_10', 'siding_20', 'unknown_00']\n",
      "4105 images.\n",
      "\n",
      "['resources/data_for_training_06/brick_10\\\\10_13306.jpeg'\n",
      " 'resources/data_for_training_06/brick_10\\\\10_13167.jpeg'\n",
      " 'resources/data_for_training_06/siding_20\\\\20_14522.jpg'\n",
      " 'resources/data_for_training_06/siding_20\\\\20_11301.jpg'\n",
      " 'resources/data_for_training_06/siding_20\\\\20_9303.jpg'\n",
      " 'resources/data_for_training_06/siding_20\\\\20_11887.jpg'\n",
      " 'resources/data_for_training_06/brick_10\\\\10_13271.jpeg'\n",
      " 'resources/data_for_training_06/brick_10\\\\10_13018.jpeg'\n",
      " 'resources/data_for_training_06/brick_10\\\\10_9747.jpeg'\n",
      " 'resources/data_for_training_06/brick_10\\\\10_12591.jpeg']\n",
      "2463 training images.\n",
      "['resources/data_for_training_06/brick_10\\\\10_13163.jpeg'\n",
      " 'resources/data_for_training_06/brick_10\\\\10_12536.jpeg'\n",
      " 'resources/data_for_training_06/brick_10\\\\10_13398.jpeg'\n",
      " 'resources/data_for_training_06/brick_10\\\\10_9143.jpeg'\n",
      " 'resources/data_for_training_06/brick_10\\\\10_9098.jpeg'\n",
      " 'resources/data_for_training_06/siding_20\\\\20_11353.jpg'\n",
      " 'resources/data_for_training_06/siding_20\\\\20_14538.jpg'\n",
      " 'resources/data_for_training_06/siding_20\\\\20_14502.jpg'\n",
      " 'resources/data_for_training_06/unknown_00\\\\00_209.jpg'\n",
      " 'resources/data_for_training_06/brick_10\\\\10_12584.jpeg']\n",
      "821 validation images.\n",
      "['resources/data_for_training_06/brick_10\\\\10_463.jpeg'\n",
      " 'resources/data_for_training_06/brick_10\\\\10_305.jpeg'\n",
      " 'resources/data_for_training_06/unknown_00\\\\00_443.jpg'\n",
      " 'resources/data_for_training_06/unknown_00\\\\00_9268.jpg'\n",
      " 'resources/data_for_training_06/brick_10\\\\10_12870.jpeg'\n",
      " 'resources/data_for_training_06/brick_10\\\\10_13379.jpeg'\n",
      " 'resources/data_for_training_06/brick_10\\\\10_12682.jpeg'\n",
      " 'resources/data_for_training_06/siding_20\\\\20_11584.jpg'\n",
      " 'resources/data_for_training_06/brick_10\\\\10_13717.jpeg'\n",
      " 'resources/data_for_training_06/brick_10\\\\10_12646.jpeg']\n",
      "821 test images.\n"
     ]
    }
   ],
   "source": [
    "def load_dataset(path):\n",
    "    data = load_files(path)\n",
    "    image_files = np.array(data['filenames'])\n",
    "    image_targets = np_utils.to_categorical(np.array(data['target']), number_of_image_categories)\n",
    "    return image_files, image_targets\n",
    "\n",
    "image_files, image_targets = load_dataset('resources/data_for_training_06/')\n",
    "\n",
    "trains_validate_files, test_files, trains_validate_targets, test_targets = \\\n",
    "    train_test_split(image_files, image_targets, test_size=0.2, random_state=42)\n",
    "\n",
    "train_files, valid_files, train_targets, valid_targets = \\\n",
    "    train_test_split(trains_validate_files, trains_validate_targets, test_size=0.25, random_state=42)\n",
    "\n",
    "print(image_names)\n",
    "\n",
    "print('%s images.\\n' % len(np.hstack([train_files, valid_files, test_files])))\n",
    "print(train_files[:10])\n",
    "print('%d training images.' % len(train_files))\n",
    "print(valid_files[:10])\n",
    "print('%d validation images.' % len(valid_files))\n",
    "print(test_files[:10])\n",
    "print('%d test images.'% len(test_files))\n",
    "\n",
    "# print(train_targets[:10])\n",
    "# print(valid_tensors[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define function for preprocessing images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_to_tensor(img_path):\n",
    "\n",
    "    img = image.load_img(img_path, grayscale=False, color_mode=\"rgb\", target_size=(400, 400), interpolation=\"nearest\")\n",
    "    img_array = image.img_to_array(img)\n",
    "\n",
    "    return np.expand_dims(img_array, axis=0)\n",
    "\n",
    "def paths_to_tensor(img_paths):\n",
    "    \n",
    "    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n",
    "    return np.vstack(list_of_tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 2463/2463 [00:20<00:00, 117.83it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 821/821 [00:07<00:00, 106.45it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 821/821 [00:06<00:00, 117.42it/s]\n"
     ]
    }
   ],
   "source": [
    "from PIL import ImageFile                            \n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True                 \n",
    "\n",
    "# pre-process the data for Keras\n",
    "train_tensors = paths_to_tensor(train_files).astype('float32')/255\n",
    "valid_tensors = paths_to_tensor(valid_files).astype('float32')/255\n",
    "test_tensors = paths_to_tensor(test_files).astype('float32')/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 400, 400, 4)       52        \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 200, 200, 4)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 200, 200, 8)       136       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 100, 100, 8)       0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 100, 100, 8)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 100, 100, 12)      396       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 50, 50, 12)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 50, 50, 12)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 50, 50, 16)        784       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 25, 25, 16)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 25, 25, 16)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 10000)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               2560256   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 771       \n",
      "=================================================================\n",
      "Total params: 2,562,395\n",
      "Trainable params: 2,562,395\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(filters=4, kernel_size=2, padding='same',\n",
    "                 activation='relu', input_shape=(400, 400, 3)))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "\n",
    "model.add(Conv2D(filters=8, kernel_size=2, padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Conv2D(filters=12, kernel_size=2, padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(filters=16, kernel_size=2, padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "orig_stdout = sys.stdout\n",
    "f = open('model_characteristics.txt', 'w')\n",
    "sys.stdout = f\n",
    "print(model.summary())\n",
    "sys.stdout = orig_stdout\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model with training and validating images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.9579 - accuracy: 0.5254\n",
      "Epoch 00001: val_loss improved from inf to 0.88371, saving model to saved_models\\weights.best.image_classifier.hdf5\n",
      "39/39 [==============================] - 55s 1s/step - loss: 0.9579 - accuracy: 0.5254 - val_loss: 0.8837 - val_accuracy: 0.5761\n",
      "Epoch 2/15\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.8276 - accuracy: 0.6171\n",
      "Epoch 00002: val_loss improved from 0.88371 to 0.78233, saving model to saved_models\\weights.best.image_classifier.hdf5\n",
      "39/39 [==============================] - 51s 1s/step - loss: 0.8276 - accuracy: 0.6171 - val_loss: 0.7823 - val_accuracy: 0.6602\n",
      "Epoch 3/15\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.7442 - accuracy: 0.6732\n",
      "Epoch 00003: val_loss improved from 0.78233 to 0.74520, saving model to saved_models\\weights.best.image_classifier.hdf5\n",
      "39/39 [==============================] - 54s 1s/step - loss: 0.7442 - accuracy: 0.6732 - val_loss: 0.7452 - val_accuracy: 0.6650\n",
      "Epoch 4/15\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.7028 - accuracy: 0.6991\n",
      "Epoch 00004: val_loss improved from 0.74520 to 0.68529, saving model to saved_models\\weights.best.image_classifier.hdf5\n",
      "39/39 [==============================] - 55s 1s/step - loss: 0.7028 - accuracy: 0.6991 - val_loss: 0.6853 - val_accuracy: 0.7454\n",
      "Epoch 5/15\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.6286 - accuracy: 0.7442\n",
      "Epoch 00005: val_loss improved from 0.68529 to 0.64380, saving model to saved_models\\weights.best.image_classifier.hdf5\n",
      "39/39 [==============================] - 55s 1s/step - loss: 0.6286 - accuracy: 0.7442 - val_loss: 0.6438 - val_accuracy: 0.7320\n",
      "Epoch 6/15\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.5619 - accuracy: 0.7763\n",
      "Epoch 00006: val_loss improved from 0.64380 to 0.64059, saving model to saved_models\\weights.best.image_classifier.hdf5\n",
      "39/39 [==============================] - 55s 1s/step - loss: 0.5619 - accuracy: 0.7763 - val_loss: 0.6406 - val_accuracy: 0.7320\n",
      "Epoch 7/15\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.5120 - accuracy: 0.8006\n",
      "Epoch 00007: val_loss did not improve from 0.64059\n",
      "39/39 [==============================] - 56s 1s/step - loss: 0.5120 - accuracy: 0.8006 - val_loss: 0.6504 - val_accuracy: 0.7150\n",
      "Epoch 8/15\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.4708 - accuracy: 0.8019\n",
      "Epoch 00008: val_loss improved from 0.64059 to 0.57426, saving model to saved_models\\weights.best.image_classifier.hdf5\n",
      "39/39 [==============================] - 60s 2s/step - loss: 0.4708 - accuracy: 0.8019 - val_loss: 0.5743 - val_accuracy: 0.7856\n",
      "Epoch 9/15\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.4061 - accuracy: 0.8425\n",
      "Epoch 00009: val_loss improved from 0.57426 to 0.56082, saving model to saved_models\\weights.best.image_classifier.hdf5\n",
      "39/39 [==============================] - 63s 2s/step - loss: 0.4061 - accuracy: 0.8425 - val_loss: 0.5608 - val_accuracy: 0.7722\n",
      "Epoch 10/15\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.3570 - accuracy: 0.8546\n",
      "Epoch 00010: val_loss improved from 0.56082 to 0.52304, saving model to saved_models\\weights.best.image_classifier.hdf5\n",
      "39/39 [==============================] - 65s 2s/step - loss: 0.3570 - accuracy: 0.8546 - val_loss: 0.5230 - val_accuracy: 0.7942\n",
      "Epoch 11/15\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.3393 - accuracy: 0.8624\n",
      "Epoch 00011: val_loss improved from 0.52304 to 0.51631, saving model to saved_models\\weights.best.image_classifier.hdf5\n",
      "39/39 [==============================] - 67s 2s/step - loss: 0.3393 - accuracy: 0.8624 - val_loss: 0.5163 - val_accuracy: 0.7795\n",
      "Epoch 12/15\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.2923 - accuracy: 0.8883\n",
      "Epoch 00012: val_loss did not improve from 0.51631\n",
      "39/39 [==============================] - 68s 2s/step - loss: 0.2923 - accuracy: 0.8883 - val_loss: 0.5734 - val_accuracy: 0.7674\n",
      "Epoch 13/15\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.2660 - accuracy: 0.8965\n",
      "Epoch 00013: val_loss did not improve from 0.51631\n",
      "39/39 [==============================] - 74s 2s/step - loss: 0.2660 - accuracy: 0.8965 - val_loss: 0.6040 - val_accuracy: 0.7795\n",
      "Epoch 14/15\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.2399 - accuracy: 0.9091\n",
      "Epoch 00014: val_loss did not improve from 0.51631\n",
      "39/39 [==============================] - 69s 2s/step - loss: 0.2399 - accuracy: 0.9091 - val_loss: 0.5841 - val_accuracy: 0.7808\n",
      "Epoch 15/15\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.2168 - accuracy: 0.9249\n",
      "Epoch 00015: val_loss did not improve from 0.51631\n",
      "39/39 [==============================] - 69s 2s/step - loss: 0.2168 - accuracy: 0.9249 - val_loss: 0.5846 - val_accuracy: 0.7759\n"
     ]
    }
   ],
   "source": [
    "epochs = 15\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.image_classifier.hdf5',\n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "history = model.fit(train_tensors, train_targets, validation_data=(valid_tensors, valid_targets),\n",
    "          epochs=epochs, batch_size=64, callbacks=[checkpointer], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_weights('saved_models/final_model.hdf5')\n",
    "\n",
    "# #Save the model\n",
    "# # serialize model to JSON\n",
    "# model_json = model.to_json()\n",
    "# with open(\"saved_models/final_model.json\", \"w\") as json_file:\n",
    "#     json_file.write(model_json)\n",
    "# # serialize weights to HDF5\n",
    "# model.save_weights(\"final_model.h5\")\n",
    "# print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 78.0755%\n"
     ]
    }
   ],
   "source": [
    "predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\n",
    "\n",
    "test_accuracy = 100*np.sum(np.array(predictions)==np.argmax(test_targets, axis=1))/len(predictions)\n",
    "print('Test accuracy: %.4f%%' % test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "brick images:\n",
      "brick_5901.jpg [[0.11266966 0.16648339 0.7208469 ]]\n",
      "brick_5902.jpg [[0.21036744 0.41694954 0.372683  ]]\n",
      "brick_62.jpg [[0.8759119  0.07978974 0.0442984 ]]\n",
      "brick_6449.jpg [[0.05328954 0.25761682 0.6890936 ]]\n",
      "brick_7266.jpg [[0.9861878  0.00342421 0.010388  ]]\n",
      "brick_7664.jpg [[0.07840574 0.10913063 0.8124637 ]]\n",
      "brick_7839.jpg [[0.34619752 0.08249631 0.5713062 ]]\n",
      "brick_81.jpg [[0.39200312 0.5754307  0.03256615]]\n",
      "\n",
      "siding images:\n",
      "siding_6450.jpg [[0.06331406 0.7321946  0.20449129]]\n",
      "siding_6672.jpg [[0.00959935 0.25119486 0.7392058 ]]\n",
      "siding_7135.jpg [[0.3977572  0.33235082 0.26989198]]\n",
      "siding_7271.jpg [[0.2231364  0.55874914 0.2181144 ]]\n",
      "siding_7488.jpg [[0.3390864  0.49201936 0.16889426]]\n",
      "siding_7493.jpg [[0.14569207 0.6666865  0.1876214 ]]\n",
      "siding_7494.jpg [[0.2514142  0.43927395 0.30931184]]\n",
      "siding_7581.jpg [[0.09551226 0.56354827 0.34093946]]\n",
      "siding_7669.jpg [[0.11048963 0.64965045 0.23985992]]\n",
      "\n",
      "unknown images:\n",
      "unknown_6673.jpg [[0.0268856 0.1931776 0.7799368]]\n",
      "unknown_6675.jpg [[0.27147985 0.30501437 0.42350575]]\n",
      "unknown_6819.jpg [[0.15737034 0.6402245  0.20240511]]\n"
     ]
    }
   ],
   "source": [
    "print(\"brick images:\")\n",
    "for image in os.listdir('test_imgs'):\n",
    "    img = plt.imread('test_imgs' + '/' + image)\n",
    "    resized_image = resize(img, (400,400,3))\n",
    "    predictions = model.predict(np.array([resized_image]))\n",
    "    if image.split(\"_\")[0] == \"brick\":\n",
    "        print(image, predictions)\n",
    "print()\n",
    "print(\"siding images:\")\n",
    "for image in os.listdir('test_imgs'):\n",
    "    img = plt.imread('test_imgs' + '/' + image)\n",
    "    resized_image = resize(img, (400,400,3))\n",
    "    predictions = model.predict(np.array([resized_image]))\n",
    "    if image.split(\"_\")[0] == \"siding\":\n",
    "        print(image, predictions)\n",
    "\n",
    "print()\n",
    "print(\"unknown images:\")\n",
    "for image in os.listdir('test_imgs'):\n",
    "    img = plt.imread('test_imgs' + '/' + image)\n",
    "    resized_image = resize(img, (400,400,3))\n",
    "    predictions = model.predict(np.array([resized_image]))\n",
    "    if image.split(\"_\")[0] == \"unknown\":\n",
    "        print(image, predictions)\n",
    "\n",
    "#['brick_10/', 'siding_20/', 'unknown_00/']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_guess(path, house_type): #brick, siding, or unknown\n",
    "    classification = ['brick', 'siding', 'unknown']\n",
    "    print()\n",
    "    print(f'Predictions for {house_type}: Percentage accuracy of model versus a human')\n",
    "    print()\n",
    "    index = classification.index(house_type)\n",
    "    correct = 0\n",
    "    wrong = 0\n",
    "    total_images = 0\n",
    "    for image in os.listdir(path):  \n",
    "        if image.split(\"_\")[0] == house_type:\n",
    "            total_images += 1\n",
    "            img = plt.imread('test_imgs' + '/' + image)\n",
    "            resized_image = resize(img, (400,400,3))\n",
    "            predictions = model.predict(np.array([resized_image]))\n",
    "            if predictions[0][index] == predictions[0].max():\n",
    "                print(f\"\\n{image}: Correct\")\n",
    "                correct += 1\n",
    "            else:\n",
    "                print(f\"\\n{image}: Wrong\")\n",
    "                wrong += 1\n",
    "            for i, prediction in enumerate(predictions[0]):\n",
    "                print('\\t',classification[i], round(100*prediction,0),'%')\n",
    "    print()\n",
    "    print('Stats:')\n",
    "    print('\\tTotal images:', total_images)\n",
    "    print('\\tAccuracy:', round(100*correct/(correct+wrong),0),'%')\n",
    "    print('\\tCorrect:', correct)\n",
    "    print('\\tWrong:', wrong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_guess' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-007f3787f186>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel_guess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'test_imgs'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'brick'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'model_guess' is not defined"
     ]
    }
   ],
   "source": [
    "model_guess('test_imgs', 'brick')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'best_guess': 'The model has identified Brick.', 'Brick': 'Brick: 43.0%', 'Siding': 'Siding: 34.0%', 'Unknown': 'Unknown: 34.0%'}\n"
     ]
    }
   ],
   "source": [
    "data = {}\n",
    "predictions = model.predict(np.array([resized_image]))[0]\n",
    "best_guess_index = predictions.index(max(predictions))\n",
    "classifications = {0: 'Brick', 1: 'Siding', 2: 'Unknown'}\n",
    "best_guess_category = classifications[best_guess_index]\n",
    "\n",
    "data['best_guess'] = f'The model has identified {best_guess_category}.'\n",
    "\n",
    "for i, prediction in enumerate(predictions):\n",
    "    data[classifications[i]] = f'{classifications[i]}: {round(100*prediction,0)}%'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brick 34.0%\n",
      "Siding 43.0%\n",
      "Unknown 44.0%\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_image = plt.imread('test_imgs/brick_62.jpg')\n",
    "img = plt.imshow(new_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize the model's accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['training data', 'validation data'], loc='upper left')\n",
    "plt.savefig('model_accuracy.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot history: \n",
    "plt.plot(history.history['loss'], label='(training data)')\n",
    "plt.plot(history.history['val_loss'], label='(validation data)')\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('value')\n",
    "plt.xlabel('No. epoch')\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.savefig('model_loss.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Below Code Needs Significant Modification to Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# \"\"\" order to make the most of our few training examples, we will \"augment\" them via a number of random transformations, so that at training time, our model will never see the exact same picture twice. This helps prevent overfitting and helps the model generalize better.\n",
    "\n",
    "# This can be done by configuring a number of random transformations to be performed on the images read by our ImageDataGenerator instance. Let's get started with an example:\"\"\"\n",
    "\n",
    "# from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# datagen = ImageDataGenerator(\n",
    "#       rotation_range=40,\n",
    "#       width_shift_range=0.2,\n",
    "#       height_shift_range=0.2,\n",
    "#       shear_range=0.2,\n",
    "#       zoom_range=0.2,\n",
    "#       horizontal_flip=True,\n",
    "#       fill_mode='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"These are just a few of the options available (for more, see the Keras documentation. Let's quickly go over what we just wrote:\n",
    "\n",
    "# rotation_range is a value in degrees (0–180), a range within which to randomly rotate pictures.\n",
    "# width_shift and height_shift are ranges (as a fraction of total width or height) within which to randomly translate pictures vertically or horizontally.\n",
    "# shear_range is for randomly applying shearing transformations.\n",
    "# zoom_range is for randomly zooming inside pictures.\n",
    "# horizontal_flip is for randomly flipping half of the images horizontally. This is relevant when there are no assumptions of horizontal assymmetry (e.g. real-world pictures).\n",
    "# fill_mode is the strategy used for filling in newly created pixels, which can appear after a rotation or a width/height shift.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib.image as mpimg\n",
    "\n",
    "# from tensorflow.keras.preprocessing.image import array_to_img, img_to_array, load_img\n",
    "\n",
    "# img_path = os.path.join('resources/data_for_training_06/brick_10\\\\10_13306.jpeg')\n",
    "# img = load_img(img_path, target_size=(150, 150))  # this is a PIL image\n",
    "# x = img_to_array(img)  # Numpy array with shape (150, 150, 3)\n",
    "# x = x.reshape((1,) + x.shape)  # Numpy array with shape (1, 150, 150, 3)\n",
    "\n",
    "# # The .flow() command below generates batches of randomly transformed images\n",
    "# # It will loop indefinitely, so we need to `break` the loop at some point!\n",
    "# i = 0\n",
    "# for batch in datagen.flow(x, batch_size=1):\n",
    "#   plt.figure(i)\n",
    "#   imgplot = plt.imshow(array_to_img(batch[0]))\n",
    "#   i += 1\n",
    "#   if i % 5 == 0:\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def my_alternative_validation_split_method(main_dir_path, split=0.2):\n",
    "    \n",
    "#     shutil.copytree(main_dir_path, main_dir_path + '_alt')\n",
    "    \n",
    "#     main_dir_path = main_dir_path + '_alt'\n",
    "#     train_path = main_dir_path + '/train'\n",
    "#     validation_path = main_dir_path + '/validate'\n",
    "    \n",
    "#     if os.path.isdir(train_path) or os.path.isdir(validation_path):\n",
    "#         print('You have already split your training and test data')\n",
    "        \n",
    "#     else:\n",
    "#         sub_dir_names = os.listdir(main_dir_path)\n",
    "#         os.mkdir(train_path)\n",
    "#         os.mkdir(validation_path)\n",
    "#         for sub_dir_name in sub_dir_names:\n",
    "#             name = sub_dir_name.split('_')[0]\n",
    "#             os.mkdir(train_path + '/'+ name)\n",
    "#             os.mkdir(validation_path + '/'+ name)\n",
    "#             images = os.listdir(main_dir_path +'/' + sub_dir_name)\n",
    "#             val_n = int(split * len(images))\n",
    "#             val_images = images[:val_n]\n",
    "#             for i in range(len(val_images)):\n",
    "#                 for filepath in glob(main_dir_path +'/' + sub_dir_name + '/' + val_images[i]):\n",
    "#                     shutil.move(main_dir_path +'/' + sub_dir_name + '/' + val_images[i], validation_path +'/' + name + '/' + val_images[i])\n",
    "#             training_images = os.listdir(main_dir_path +'/' + sub_dir_name)\n",
    "#             print(f'{name}: {len(images)} total images')\n",
    "#             print(f'{name}: {len(training_images)} training images')\n",
    "#             print(f'{name}: {val_n} validation images')\n",
    "#             print('validation split: ', 100 * round(val_n/(val_n + len(training_images)),2), '%')\n",
    "#             print()\n",
    "#             for i in range(len(training_images)):\n",
    "#                 for filepath in glob(main_dir_path +'/' + sub_dir_name + '/' + training_images[i]):\n",
    "#                     shutil.move(main_dir_path +'/' + sub_dir_name + '/' + training_images[i], train_path +'/' + name + '/' + training_images[i])\n",
    "#         for sub_dir_name in sub_dir_names:\n",
    "#             os.rmdir(main_dir_path +'/' + sub_dir_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main_dir = 'resources/data_for_training_06'\n",
    "\n",
    "# my_alternative_validation_split_method(main_dir, split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Adding rescale, rotation_range, width_shift_range, height_shift_range,\n",
    "# # shear_range, zoom_range, and horizontal flip to our ImageDataGenerator\n",
    "# train_datagen = ImageDataGenerator(\n",
    "#     rescale=1./255,\n",
    "#     rotation_range=40,\n",
    "#     width_shift_range=0.2,\n",
    "#     height_shift_range=0.2,\n",
    "#     shear_range=0.2,\n",
    "#     zoom_range=0.2,\n",
    "#     horizontal_flip=True,)\n",
    "\n",
    "# # Note that the validation data should not be augmented!\n",
    "# val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# # Flow training images in batches of 32 using train_datagen generator\n",
    "# train_generator = train_datagen.flow_from_directory(\n",
    "#         main_dir + '_alt' + '/train',  # This is the source directory for training images\n",
    "#         target_size=(400, 400),  \n",
    "#         batch_size=20,\n",
    "#         # Since we use binary_crossentropy loss, we need binary labels\n",
    "#         class_mode='categorical')\n",
    "\n",
    "# # Flow validation images in batches of 32 using val_datagen generator\n",
    "# validation_generator = val_datagen.flow_from_directory(\n",
    "#         main_dir + '_alt' + '/validate',\n",
    "#         target_size=(400, 400),\n",
    "#         batch_size=20,\n",
    "#         class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "\n",
    "# model.add(Conv2D(filters=4, kernel_size=2, padding='same',\n",
    "#                  activation='relu', input_shape=(400, 400, 3)))\n",
    "# model.add(MaxPooling2D(pool_size=2))\n",
    "\n",
    "# model.add(Conv2D(filters=8, kernel_size=2, padding='same', activation='relu'))\n",
    "# model.add(MaxPooling2D(pool_size=2))\n",
    "# model.add(Dropout(0.1))\n",
    "\n",
    "# model.add(Conv2D(filters=12, kernel_size=2, padding='same', activation='relu'))\n",
    "# model.add(MaxPooling2D(pool_size=2))\n",
    "# model.add(Dropout(0.2))\n",
    "\n",
    "# model.add(Conv2D(filters=16, kernel_size=2, padding='same', activation='relu'))\n",
    "# model.add(MaxPooling2D(pool_size=2))\n",
    "# model.add(Dropout(0.3))\n",
    "\n",
    "\n",
    "\n",
    "# model.add(Flatten())\n",
    "\n",
    "# model.add(Dense(256, activation='relu'))\n",
    "\n",
    "# model.add(Dropout(0.4))\n",
    "\n",
    "# model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "\n",
    "# model.summary()\n",
    "\n",
    "# model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs = 15\n",
    "\n",
    "# checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.image_classifier.hdf5',\n",
    "#                                verbose=1, save_best_only=True)\n",
    "\n",
    "# history = model.fit(train_tensors, train_targets, validation_data=(valid_tensors, valid_targets),\n",
    "#           epochs=epochs, batch_size=64, callbacks=[checkpointer], verbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
